---
title: "Environment representation in sim-to-real imitation learning based robot navigation"
excerpt: "This project studies the impact of different sensor fusion methods on an imitation-learning based control strategy. <br/>"
collection: portfolio
---

### Improving Robot Navigation with Better Sensor Data

In this project, we studied how a robot's ability to navigate from a simulated environment to the real world is affected by the way it "sees" its surroundings.

We used a **neural network** to teach a robot to avoid collisions by having it learn from sensor data collected in a simulation. Before feeding the data to the network, we combined information from different sensors.

When we deployed the robot in the real world, we tested how well it performed with different types of sensor data. Our main finding was that **categorical and semantic information**—such as identifying an object as a chair versus a table—was more important for successful navigation than simple color or texture information.

Formulation

<img src='https://hzyu17.github.io/hongzheyu.github.io/images/local_navigation_method.png'>


[Video Introduction](https://youtu.be/ucGyuMjlgEk)